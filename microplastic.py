# -*- coding: utf-8 -*-
"""microplastic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oCI-CoqhlxeLJnx6KoE9J7oNeT1w1dKj
"""

!pip install netCDF4

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.cluster import DBSCAN
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
from sklearn import datasets

import netCDF4 as nc
import pandas as pd
import numpy as np

file_path = '20181216.nc'
data = nc.Dataset(file_path)
data.variables.keys()

lat = data.variables['lat'][:]
lon = data.variables['lon'][:]
num_mp_samples = data.variables['num_mp_samples'][:]
mp_conc = data.variables['mp_concentration'][:]
time = data.variables['time'][:]

print(mp_conc)
print(time)

# extract mp_concentration from (1,297,1440) to (297,1440)
# mapping the mp_concentration to the corresponding lattitude and longitude
mp_conc_squz = np.squeeze(mp_conc)

# np.meshgrid() create a grid out of two given one-dimensional arrays
# indexing='ij' makes sure the output follows the matrix indexing (row-major ij) / Cartesian indexing ('xy')
lat_grid, lon_grid = np.meshgrid(lat, lon, indexing='ij')

# .flatten() converts multi-dimensional arrays into 1D array
# .flatten() reshapes 2D lat_grid / lon_grid array into 1D array to be stored in Pandas

print(lat_grid.shape)
print(lat_grid.flatten().shape)

df = pd.DataFrame({
    'lat': lat_grid.flatten(),
    'lon': lon_grid.flatten(),
    'mp_conc': mp_conc_squz.flatten()})
df.head()
df.shape

"""Data Cleaning"""

# check if there are NaN columns
df.isna().sum()

# select rows that contained at least one NaN
df[df.isna().any(axis=1)]

# drop the rows with NaN and double check for accuracy
df_clean = df.dropna()
df_clean.isna().sum()
df_clean.shape

"""Model Training

System crashed due to high RAM usage during training.

- already dropped NaN values
- can reduce the dataset to only focused on high threshold region (tentatively set to 14000)
"""

# threshold = np.percentile(df['mp_conc'], 90)
# print(threshold)
df_filtered = df[df['mp_conc'] > 14000].copy()

df_filtered.shape()

# standardisation for data preprocessing
scaler = StandardScaler()
X = scaler.fit_transform(df_filtered[['lat', 'lon','mp_conc']])

!pip install basemap

from mpl_toolkits.basemap import Basemap

eps = 0.5
min_samples = 5

def dbscan(df,eps, min_samples):
    dbscan = DBSCAN(eps=eps, min_samples=min_samples)
    df['cluster'] = dbscan.fit_predict(X)

    plt.figure(figsize=(10, 6))

    # Create a world map projection
    m = Basemap(projection="cyl", resolution="c", llcrnrlat=-40, urcrnrlat=40, llcrnrlon=0, urcrnrlon=360)
    m.drawcoastlines()
    m.fillcontinents(color="lightgray", zorder=0)

    unique_clusters = sorted(df['cluster'].unique())
    color = plt.get_cmap('nipy_spectral', len(unique_clusters))
    num_clusters = len([c for c in unique_clusters if c != -1])
    num_noise = sum(df['cluster'] == -1)

    for cluster in unique_clusters:
        if cluster == -1:
            continue
        cluster_data = df[df['cluster'] == cluster]
        plt.scatter(cluster_data['lon'], cluster_data['lat'], c=color(cluster), label=f'Cluster {cluster}', s=10)

    if len(unique_clusters) < 10:
        plt.legend(title="Clusters", bbox_to_anchor=(1.05, 1), loc="upper left")

    else:
        print(f"Number of Clusters: {num_clusters}")
        print(f"Number of Noise: {num_noise}")

    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.title(f'DBSCAN Clustering, eps: {eps} & min_samples: {min_samples}')
    plt.show()

dbscan(df_filtered,0.4,10)

cluster_stats = df_filtered.groupby('cluster')['mp_conc'].describe()

cluster_stats.style.format("{:.2f}")

print(df_filtered)

import seaborn as sns
def histogram(df):
  df = df[df['cluster'] != -1]

  # get unique clusters
  unique_clusters = sorted(df['cluster'].unique())
  print(unique_clusters)

  # define number of subplots
  num_clusters = len(unique_clusters)
  cols = 3
  rows = rows = (num_clusters // cols) + (num_clusters % cols > 0)

  # create subplots
  fig, axes = plt.subplots(rows, cols, figsize=(15, rows*4))
  axes = axes.flatten()

  for i, cluster in enumerate(unique_clusters):

    ax = axes[i]
    subset = df[df['cluster'] == cluster]['mp_conc']

    color = plt.cm.nipy_spectral(i / max(1, len(unique_clusters)-1))
    sns.histplot(subset, kde=True, ax=ax, color=color)
    ax.set_xlabel("Microplastic Concentration")
    ax.set_ylabel("Frequency")
    ax.set_title(f"Cluster {cluster} MP Distribution")

  for j in range(i+1, len(axes)):
    fig.delaxes(axes[j])

  plt.tight_layout()
  plt.show()

histogram(df_filtered)

"""plotting bounding box
1. comput bounding box with min and max latitude and longitude for each cluster
2. plot bounding box
3. overlay cluster points to show spatial distribution
"""

import matplotlib.patches as patches
def bounding_boxes(df):
  df = df[df['cluster'] != -1]

  # compute bounding box
  bounding_df = df[df['cluster'] != -1].groupby('cluster').agg(
      min_lat = ('lat','min'),
      max_lat = ('lat','max'),
      min_lon = ('lon','min'),
      max_lon = ('lon','max'),
      count = ('cluster','count')
  ).reset_index()


  # initialise world map
  fig, ax = plt.subplots(figsize=(12,6))

  m = Basemap(projection="cyl", resolution="c", llcrnrlat=-40, urcrnrlat=40, llcrnrlon=0, urcrnrlon=360)
  m.drawcoastlines()
  m.fillcontinents(color="lightgray", zorder=0)

  # assign colors to clusters
  unique_clusters = bounding_df['cluster'].unique()
  cmap = plt.get_cmap('nipy_spectral', len(unique_clusters))
  cluster_colors = {cluster: cmap(i/max(1, len(unique_clusters)-1)) for i, cluster in enumerate(unique_clusters)}

  for cluster in unique_clusters:
    cluster_data = df[df['cluster'] == cluster]
    ax.scatter(cluster_data['lon'], cluster_data['lat'], label=f'Cluster {cluster}', s=10, alpha=0.7, c=cluster_colors[cluster])

  for index, row in bounding_df.iterrows():
    cluster_id = row['cluster']
    min_lat, max_lat = row['min_lat'], row['max_lat']
    min_lon, max_lon = row['min_lon'], row['max_lon']
    bbox_color = cluster_colors[cluster_id]

    # create rectangle bounding box
    rect = patches.Rectangle((min_lon, min_lat),
                            max_lon - min_lon,
                            max_lat - min_lat,
                            linewidth=2, edgecolor=bbox_color,fill=False)
    ax.add_patch(rect)

  plt.legend(title="Clusters", bbox_to_anchor=(1.05, 1), loc="upper left")
  plt.xlabel('Longitude')
  plt.ylabel('Latitude')
  plt.title ('Cluster Bounding Boxes')
  plt.show()


bounding_boxes(df_filtered)

"""Zoom into each bounding box to look at the microplastic distribution in details"""

def plot_cluster_map(df, cluster_id):
  """
    Zooms into a selected cluster and visualizes microplastic concentration distribution.

    - Uses bounding box to filter the cluster region.
    - Scatter plot with colorbar to show concentration levels.

    Parameters:
    - df (DataFrame): Dataset with 'latitude', 'longitude', 'mp_conc', and 'cluster'.
    - cluster_id (int): The cluster to zoom into.

    Output:
    - Scatter plot of microplastic concentration for the selected cluster.
    """
  cluster_bounds = df.groupby('cluster').agg(
      max_lat = ('lat','max'),
      min_lat = ('lat','min'),
      max_lon = ('lon','max'),
      min_lon = ('lon','min'),
      count = ('cluster','count')
  ).reset_index()


  if cluster_id not in cluster_bounds['cluster'].values:
    print(f"Cluster {cluster_id} not found in the dataset.")
    return

  row = cluster_bounds[cluster_bounds['cluster'] == cluster_id]
  min_lon, max_lon = row['min_lon'].values[0], row['max_lon'].values[0]
  min_lat, max_lat = row['min_lat'].values[0], row['max_lat'].values[0]
  count = row['count'].values[0]

  # select points that belong to the clusters
  df_cluster = df[df["cluster"] == cluster_id]
  cluster_stats = df_cluster['mp_conc'].describe()

   # Format stats as a string
  stats_text = (
        f"Cluster {cluster_id} Stats:\n"
        f"Count: {int(cluster_stats['count'])}\n"
        f"Mean: {cluster_stats['mean']:.2f}\n"
        f"Std Dev: {cluster_stats['std']:.2f}\n"
        f"Min: {cluster_stats['min']:.2f}\n"
        f"25%: {cluster_stats['25%']:.2f}\n"
        f"50% (Median): {cluster_stats['50%']:.2f}\n"
        f"75%: {cluster_stats['75%']:.2f}\n"
        f"Max: {cluster_stats['max']:.2f}"
    )

  # decide plotting span
  # Step 1: Compute lat/lon span
  lat_span = max_lat - min_lat
  lon_span = max_lon - min_lon

  # Step 2: Expand bounds using percentage (10% of current range)
  expand_ratio = 0.20  # Adjust this for more or less padding

  lat_expand = lat_span * expand_ratio
  lon_expand = lon_span * expand_ratio

  # Create a world map projection
  fig, ax = plt.subplots(figsize=(12,6))
  m = Basemap(projection="cyl", resolution="c", llcrnrlat=min_lat - lat_expand, urcrnrlat=max_lat + lat_expand, llcrnrlon=min_lon - lon_expand, urcrnrlon=max_lon + lon_expand, ax=ax)
  m.drawcoastlines()
  m.fillcontinents(color="lightgray", zorder=0)


  scatter = plt.scatter(df_cluster['lon'], df_cluster['lat'], c=df_cluster['mp_conc'], cmap='viridis', s=15, alpha=0.7)
  cbar = plt.colorbar(scatter, ax=ax)
  cbar.set_label('Microplastic Concentration')

  ax.text(
      min_lon + (max_lon - min_lon) * 0.85,  # 85% from left
      min_lat + (max_lat - min_lat) * 0.9,
      f"Count: {count}",
      fontsize=10, color='black',
      bbox=dict(facecolor='white', alpha=0.7, edgecolor='black')
  )
  plt.figtext(
      0.1, -0.05,
      stats_text, ha="left", fontsize=10, bbox=dict(facecolor="white", alpha=0.7))

  plt.xlim(min_lon, max_lon)
  plt.ylim(min_lat, max_lat)
  plt.xlabel('Longitude')
  plt.ylabel('Latitude')
  plt.title(f'Microplastic Concentration in Cluster {cluster_id}')
  plt.show()

plot_cluster_map(df_filtered, 1)

for i in range (df_filtered['cluster'].max()):
  plot_cluster_map(df_filtered, i)

"""DBSCAN Documentation:

https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html

Run Experiments for hyperparameter tuning
iterate on different eps and min_samples combinations
"""

dbscan(df_filtered,0.8,10)

eps = [0.2,0.3]
min_samples = [3,4]

def tune_dbscan(df, eps, min_samples):
  num_rows = len(eps)
  num_cols = len(min_samples)
  fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols*4, num_rows*3))
  axes = axes.flatten()

  results = []

  for i, ep in enumerate(eps):
    for j, min_sample in enumerate(min_samples):
      # run DBSCAN
      df_copy = df.copy()
      dbscan = DBSCAN(eps=ep, min_samples=min_sample)
      scaler = StandardScaler()
      X = scaler.fit_transform(df_copy[['lat', 'lon','mp_conc']])
      df_copy['cluster'] = dbscan.fit_predict(X)

      unique_clusters = sorted(df_copy['cluster'].unique())
      num_clusters = len([c for c in unique_clusters if c != -1])
      num_noise = sum(df_copy['cluster'] == -1)

      results.append((ep, min_sample, num_clusters, num_noise))

      # plot on the right axis
      ax = axes[i * num_cols + j]
      m = Basemap(projection="cyl", resolution="c", ax=ax,
                        llcrnrlat=-40, urcrnrlat=40, llcrnrlon=0, urcrnrlon=360)
      m.drawcoastlines()
      m.fillcontinents(color="lightgray", zorder=0)
      color = plt.get_cmap('nipy_spectral', len(unique_clusters))

      for cluster in unique_clusters:
        if cluster == -1:
          continue
        cluster_data = df_copy[df_copy['cluster'] == cluster]
        ax.scatter(cluster_data['lon'], cluster_data['lat'], c=color(cluster), label=f'Cluster {cluster}', s=10)

      ax.set_title(f'eps = {ep}, min_samples = {min_sample}')
      ax.set_xlabel('Longitude')
      ax.set_ylabel('Latitude')

      ax.text(0.95, 0.95,
              f"Clusters: {num_clusters} \nNoise: {num_noise}",
              transform=ax.transAxes, fontsize=9, color='black',
              bbox=dict(facecolor='white', alpha=0.7, edgecolor='black'),
              ha='right', va='top')

  # hide additional plots
  for idx in range(len(eps) * len(min_samples), len(axes)):
        axes[idx].axis("off")

  plt.tight_layout(rect=[0, 0, 1, 0.96])
  plt.show()

  return results

tune_dbscan(df_filtered, eps, min_samples)

eps = [0.2,0.3,0.4,0.5,0.6,0.7,0.8]
min_samples = [3,4,5,6,7,8,9,10]

tune_dbscan(df_filtered, eps, min_samples)

eps = [0.2,0.3]
min_samples = [3,4,5,6,7,8,9,10]

dbscan(df_filtered,0.3,7)

bounding_boxes(df_filtered)

histogram(df_filtered)

dbscan(df_filtered, 0.2, 3)
bounding_boxes(df_filtered)
histogram(df_filtered)

dbscan(df_filtered, 0.5, 5)
bounding_boxes(df_filtered)
histogram(df_filtered)

def analyze_nc_file(file_path, threshold, eps, min_samples):
    print(f"Loading data from {file_path}...")
    dataset = nc.Dataset(file_path)

    # Extract relevant variables (modify based on actual variable names in your dataset)
    lat = dataset.variables['lat'][:]
    lon = dataset.variables['lon'][:]
    mp_conc = dataset.variables['mp_concentration'][:]
    mp_conc_squz = np.squeeze(mp_conc)

    # Convert to a DataFrame
    lat_grid, lon_grid = np.meshgrid(lat, lon, indexing='ij')
    df = pd.DataFrame({
        "lat": lat_grid.flatten(),
        "lon": lon_grid.flatten(),
        "mp_conc": mp_conc_squz.flatten()
    })

    # Handle Missing Data
    df = df.dropna()
    df_filtered = df[df['mp_conc'] > threshold].copy()

    # Standardize Data
    scaler = StandardScaler()
    X = scaler.fit_transform(df_filtered[['lat', 'lon', 'mp_conc']])

    # Run DBSCAN Clustering
    print("Running DBSCAN Clustering...")
    dbscan(df_filtered, eps, min_samples)

    # Visualize Cluster Bounding Boxes
    print("Visualizing Cluster Bounding Boxes...")
    bounding_boxes(df_filtered)

    # Plot Microplastic Concentration Histogram
    print("Plotting Histogram of Microplastic Concentration...")
    histogram(df_filtered)

    print("\n✅ Analysis Complete!")

# Example Usage
analyze_nc_file("20181216.nc", 14000, eps=0.5, min_samples=5)

analyze_nc_file("20180816.nc", 14000, eps=0.5, min_samples=5)